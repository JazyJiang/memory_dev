strategy: decoder_only

global:
  seed: 42

train:
  optim: adamw_torch
  lr_scheduler_type: cosine
  warmup_ratio: 0.01
  weight_decay: 0.001
  learning_rate: 0.0003
  epochs: 50
  per_device_batch_size: 256
  gradient_accumulation_steps: 2
  logging_step: 10
  save_and_eval_strategy: epoch
  save_and_eval_steps: 1000
  model_max_length: 2048

dataset:
  tasks: seqrec
  max_his_len: 20
  only_train_response: false
  train_prompt_sample_num: "1"
  train_data_sample_num: "-1"
  valid_prompt_id: 0
  sample_valid: true
  valid_prompt_sample_num: 2

model:
  decoder_only:
    max_seq_len: 2048
    d_model: 512
    n_layers: 6
    n_heads: 8
    head_dim: null
    n_kv_heads: null
    multiple_of: 256
    ffn_dim_multiplier: null
    ffn_dim: 2048
    dropout: 0.0
    rope_theta: 10000.0
    init_base_std: null
    init_std_factor: disabled

pkm:
  decoder_only:
    pk_is_enabled: false
    pk_layers: []              # e.g. [1, 3, 5]
    pk_mem_n_keys: 128
    pk_mem_heads: 4
    pk_mem_knn: null
    pk_mem_share_values: true
    pk_mem_k_dim: 512
    pk_mem_v_dim: -1
    pk_swilu_projection: true
    pk_value_fixed_lr: 0.001
    pk_value_weight_decay: 0.0
    pk_mem_gated: false
    pk_peer_variant: false
    pk_topk: 8
    pk_mem_dim: null
    pk_use_gating: false